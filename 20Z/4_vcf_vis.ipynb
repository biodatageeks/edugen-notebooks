{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza wariantów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                               # moduł OS języka Python\n",
    "user_name = os.environ.get('USER')      # pobieramy zmienną środowiskową USER\n",
    "bucket = f\"gs://edugen-lab-{user_name}\" # konstruujemy sciezke dostepowa do pliku\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzenie czy dane są dostępne\n",
    "! gsutil ls -r $bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie sesji Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stworzenie tabeli z wariantami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_path=f\"{bucket}/vcf/mother10.vcf\"\n",
    "table_var = 'variants'\n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_var}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_var} \\\n",
    "USING org.biodatageeks.sequila.datasources.VCF.VCFDataSource \\\n",
    "OPTIONS(path \"{var_path}\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weryfikacja danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Zadanie 4_1:</b> \n",
    "Napisz polecenie, które pokaże strukturę (kolumny i ich typy) tabeli z wariantami\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'select count (*) from {table_var}').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że ALT jest rodzaju tablicowego, sprawdźmy, czy są warianty mutlialleliczne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'select * from {table_var} where size(alt) > 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'select * from {table_var} where size(alt) > 1').count() # ile ich jest ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'select genotypes from {table_var}').show(truncate=False) # tabela w postaci nieznormalizowanej -> mamy listę wartości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy warianty multialleliczne. Zeby rozbic je na warianty bi-alleliczne uzyjemy narzedzia VT uruchamianego na kilku executorach. Importujemy potrzebny moduł seqtender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyseqtender import SeqTenderAnnotation\n",
    "seq_anno = SeqTenderAnnotation(spark)\n",
    "dec_variants = seq_anno.pipe_variants (var_path, \"vt decompose -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzimy liczność wyniku\n",
    "dec_variants.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zapiszemy wyniki w kubełku\n",
    "var_dec_path = f\"{bucket}/vcf/mother_dec.vcf\"\n",
    "seq_anno.save_variants (var_dec_path, dec_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_var_dec = 'variants_dec'\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_var_dec}')\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_var_dec} \\\n",
    "USING org.biodatageeks.sequila.datasources.VCF.VCFDataSource \\\n",
    "OPTIONS(path \"{var_path}\")')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_var_dec}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zrealizujemy jeszcze proces adnotacji wariantów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyseqtender import SeqTenderAnnotation\n",
    "seq_anno = SeqTenderAnnotation(spark)\n",
    "\n",
    "vcf_path=f\"{bucket}/vcf/mother_dec.vcf\"\n",
    "\n",
    "cache_dir = \"/mnt/data/annotation/102.0\"\n",
    "vep_version=\"102\"\n",
    "annotate_cmd = f\"\"\"vep --dir {cache_dir} --pick_allele --format vcf --no_stats --force_overwrite  --uniprot  -cache --vcf -offline --assembly GRCh37\n",
    "        -o stdout \"\"\".replace(\"\\n   \", \"\")\n",
    "\n",
    "annotated = seq_anno.pipe_variants(vcf_path, annotate_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_anno_path = f\"{bucket}/vcf/mother_anno.vcf\"\n",
    "seq_anno.save_variants (var_anno_path, annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_anno_path=f\"{bucket}/vcf/mother_anno.vcf\"\n",
    "table_var_anno= 'variants_anno'\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_var_anno}')\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_var_anno} \\\n",
    "USING org.biodatageeks.sequila.datasources.VCF.VCFDataSource \\\n",
    "OPTIONS(path \"{var_anno_path}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = spark.sql(f\"select * from {table_var_anno}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Zadanie 4_2:</b> \n",
    "Wyświetl schemat DF anno i podaj liczbę kolumn. Jakiego typu są nowe kolumny?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.sql.functions import *\n",
    "anno1 = anno.withColumn(\"INFO_CSQ2\", explode((\"INFO_CSQ\")))\n",
    "anno2 = anno1.selectExpr( \"*\", \" INFO_CSQ2.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Zadanie 4_3:</b> \n",
    "Pogrupuj warianty po kolumnie IMPACT oraz oblicz ile jest wariantów w każdej grupie. Zrób to samo dla kolumny Consequence oraz SYMBOL.\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteka Pandas\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Moduł Pandas jest biblioteką Pythonową do manipulacji danymi. W szczegolnosci w pandas mozemy stworzyc ramki danych i wykonywac na niej analize, agregacje oraz wizualizacje danych. \n",
    "Przy nieduzych zbiorach danych i prostych operacjach to doskonała biblioteka. Jednak kiedy zbior danych sie rozrasta lub kiedy wymagane sa zlozone transformacje to operacje moga byc wolne.\n",
    "\n",
    "Operacje na rozproszonych danych sa szybsze. Ale tu takze napotykamy ograniczenia np trudność w wizualizacji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_var_anno} LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ważne** Metoda toPandas() na ramce pyspark, konwertuje ramkę pyspark do ramki pandas. Wykonuje akcje pobrania wszystkich danych z executorów (z JVM) i transfer do  programu sterujacego (driver) i konwersje do typu Pythonowego w notatniku. Ze względu na ograniczenia pamięciowe w programie sterującym należy to wykonywać na podzbiorach danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie pokazują się wszystkie kolumny (w środku mamy ...). Aby wyświetlić całość musimy ustawić:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_pandas = anno2.toPandas()\n",
    "anno_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib\n",
    "https://pandas.pydata.org/pandas-docs/version/0.23.4/api.html#api-dataframe-plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "anno2.groupby(\"ref\").count().toPandas().plot.bar(x='ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno2.groupby(\"ref\").count().toPandas().plot.pie(y='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Zadanie 4_4:</b> \n",
    "Przerób powyższe dwa rysunki, tak żeby pokazywały rozkład referencji tylko dla SNV (bez indeli).\n",
    "    </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Zadanie 4_5:</b> \n",
    "Przygotuj pie chart dla kolumny Consequences. Uwzględnij jedną konsekwencję dla każdego wariantu.\n",
    "    </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edugen",
   "language": "python",
   "name": "edugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
