{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odczyty uliniowione\n",
    "\n",
    "W tej części zajmiemy się analiża uliniowionych odczytów (BAM) poprzez platformę Spark. Tym razem zamiast korzystać z API DataFrame posłużymy się językiem SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://edugen-lab-agaszmurlo/bam/\n",
      "gs://edugen-lab-agaszmurlo/bam/.DS_Store\n",
      "gs://edugen-lab-agaszmurlo/bam/father.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/father.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother.bam.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother.bam.sbi\n",
      "gs://edugen-lab-agaszmurlo/bam/mother10.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother10.bam.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother10.bam.sbi\n",
      "gs://edugen-lab-agaszmurlo/bam/mother2.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother2.bam.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother2.bam.sbi\n",
      "gs://edugen-lab-agaszmurlo/bam/mother3.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother3.bam.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother3.bam.sbi\n",
      "gs://edugen-lab-agaszmurlo/bam/mother4.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother4.bam.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother4.bam.sbi\n",
      "gs://edugen-lab-agaszmurlo/bam/mother6.bam\n",
      "gs://edugen-lab-agaszmurlo/bam/mother6.bam.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/mother6.bam.sbi\n",
      "gs://edugen-lab-agaszmurlo/bam/son.bai\n",
      "gs://edugen-lab-agaszmurlo/bam/son.bam\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls gs://edugen-lab-$USER/bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie sesji Spark\n",
    "Zainicjowanie sesji Spark oraz stworzenie schematu bazy danych z której będziemy korzystać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://edugen-lab-agaszmurlo\n"
     ]
    }
   ],
   "source": [
    "import os                               # moduł OS języka Python\n",
    "user_name = os.environ.get('USER')      # pobieramy zmienną środowiskową USER\n",
    "bucket = f\"gs://edugen-lab-{user_name}\" # konstruujemy sciezke dostepowa do pliku\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy korzystać z modułu Spark SQL. Możemy nasze dane traktować jako dane tabelaryczne. Zdefiniujmy tabelę.\n",
    "Tym razem korzystamy z DataSource -> BAMDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_bam =  'alignments'\n",
    "\n",
    "alignments_path = f\"{bucket}/bam/mother10.bam\"\n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_bam}')\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_bam} \\\n",
    "USING org.biodatageeks.sequila.datasources.BAM.BAMDataSource \\\n",
    "OPTIONS(path \"{alignments_path}\")')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "| col_name|data_type|comment|\n",
      "+---------+---------+-------+\n",
      "|sample_id|   string|   null|\n",
      "|    qname|   string|   null|\n",
      "|     flag|      int|   null|\n",
      "|   contig|   string|   null|\n",
      "|      pos|      int|   null|\n",
      "|pos_start|      int|   null|\n",
      "|  pos_end|      int|   null|\n",
      "|     mapq|      int|   null|\n",
      "|    cigar|   string|   null|\n",
      "|    rnext|   string|   null|\n",
      "|    pnext|      int|   null|\n",
      "|     tlen|      int|   null|\n",
      "|      seq|   string|   null|\n",
      "|     qual|   string|   null|\n",
      "|   tag_AM|      int|   null|\n",
      "|   tag_AS|      int|   null|\n",
      "|   tag_BC|   string|   null|\n",
      "|   tag_BQ|   string|   null|\n",
      "|   tag_BZ|   string|   null|\n",
      "|   tag_CB|   string|   null|\n",
      "+---------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"describe {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby dostać się do danych w tabeli, bedziemy korzystać z język SQL (standard ANSI). \n",
    "Struktura polecen SQL jest nastepująca: \n",
    "\n",
    "```sql\n",
    "SELECT kolumny\n",
    "FROM tabela\n",
    "WHERE warunki\n",
    "GROUP BY wyrazenia grupujace\n",
    "HAVING warunki na grupy\n",
    "ORDER BY wyrazenie sortujące\n",
    "```\n",
    "Zapytanie może mieć podzapytania zagnieżdzone. W zapytaniu można korzystać ze złączeń z innymi tabeli poprzez JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----+------+-------+---------+-------+----+-------------------+-----+--------+----+--------------------+--------------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+---------+--------------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+--------------------+------+------+------+------+\n",
      "|sample_id|               qname|flag|contig|    pos|pos_start|pos_end|mapq|              cigar|rnext|   pnext|tlen|                 seq|                qual|tag_AM|tag_AS|tag_BC|tag_BQ|tag_BZ|tag_CB|tag_CC|tag_CG|tag_CM|tag_CO|tag_CP|tag_CQ|tag_CR|tag_CS|tag_CT|tag_CY|tag_E2|tag_FI|tag_FS|tag_FZ|tag_H0|tag_H1|tag_H2|tag_HI|tag_IH|tag_LB|   tag_MC|              tag_MD|tag_MI|tag_MQ|tag_NH|tag_NM|tag_OA|tag_OC|tag_OP|tag_OQ|tag_OX|tag_PG|tag_PQ|tag_PT|tag_PU|tag_Q2|tag_QT|tag_QX|tag_R2|tag_RG|tag_RX|              tag_SA|tag_SM|tag_TC|tag_U2|tag_UQ|\n",
      "+---------+--------------------+----+------+-------+---------+-------+----+-------------------+-----+--------+----+--------------------+--------------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+---------+--------------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+--------------------+------+------+------+------+\n",
      "| mother10|H0164ALXX140820:2...| 129|    20|  83235|    83235|  83303|   0|           4S69M78S|   20|15931644|  69|TCTTTCTCTCTCTCTCT...|:;=::=?:?8>=?9?=>...|  null|    69|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  69|  null|    60|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,7416672,-,59S3...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2169|    20| 293889|   293889| 293920|   0|            32M119H|   20|  293889|  32|TTGTTTCTTTTTTCTTT...|############<4;.:...|  null|    32|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     null|                  32|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10141234,+,77M...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...| 129|    20| 465705|   465705| 465753|   7|          21S49M81S|   20|10024050|  49|TGGGCAACAGAGTGAGA...|;96:;?><>9=9;7=6=...|  null|    49|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  49|  null|    60|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,32947973,+,40M...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2227|    20| 465710|   465710| 465755|   0|          35H46M70H|   20|10156476|  46|CAAAAAAAAAAAAAAAA...|+,8,,,?,>?????>>?...|  null|    46|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  46|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10156308,+,89M...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2179|    20| 723766|   723766| 723832|   0|          71H67M13H|   20|10132488|  67|TCTCTCTCTCTCTCTCT...|@=?:@;@?0;@?@;<?=...|  null|    65|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                64A2|  null|  null|  null|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10132723,-,59S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2179|    20| 723766|   723766| 723835|   0|          59H70M22H|   20|10132434|  70|TCTCTCTCTCTCTCTCT...|?:7?==?9@><??9@:?...|  null|    65|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                64A5|  null|  null|  null|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10132723,-,71S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2179|    20| 723766|   723766| 723832|   6|          58H67M26H|   20|10132477|  67|TCTCTCTCTCTCTCTCT...|?:?=09@-@?@?@???@...|  null|    67|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  67|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10132723,-,70S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...| 145|    20|1543878|  1543878|1543985|   0|            43S108M|   20|10008309| 108|AGTTCTTAAAATCCATT...|;9;.><-:--6-*,8;=...|  null|    63|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|31A8T1A7T0C7C0A4T...|  null|    60|  null|     9|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10008610,-,83M...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2179|    20|1648876|  1648876|1648928|   0|95H24M4I12M1I7M2D8M|   20|10045986|  53|TCTTTCTTTCTTTCTTT...|@-08>-@,;5@;?6<8>...|  null|    30|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|              43^TC8|  null|  null|  null|     7|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10046248,-,29S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2115|    20|2058293|  2058293|2058323|   6|          77H31M43H|   20|16041390|  31|TTTTTTTTTTTTTTTTT...|==<<;<;;=9==,=,5,...|  null|    31|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  10H141M|                  31|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,16041555,-,62S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2161|    20|2522293|  2522293|2522335|   0|          27H43M81H|   20|10144245|  43|AAAAAAAAAAAAAAAAA...|#################...|  null|    38|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|   78H73M|               13G29|  null|  null|  null|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10144245,-,78S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2195|    20|2806380|  2806380|2806409|  52|         102H30M19H|   20|10011298|  30|TCAATAAAAATAAAAAA...|:+9,48,,13++,=87,...|  null|    30|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  30|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10011428,-,29S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2131|    20|2831118|  2831118|2831171|   0|             97H54M|   20|10066166|  54|AAAAATTATCTGGGCAT...|,07:3;?8<7-16>*,>...|  null|    34|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  16H135M|         14T0G5G25G6|  null|  null|  null|     4|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10066389,-,74M...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2115|    20|2849265|  2849265|2849299|   3|          77H35M39H|   20|16041315|  35|TTTTTTTTTTTTTTTTT...|==<;:<<<======7<=...|  null|    35|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|6H132M13H|                  35|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,16041555,-,61S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2169|    20|2989376|  2989376|2989405|   0|          98H30M23H|   20| 2989376|  30|AAAAAAAAAAAAAAACA...|################<...|  null|    30|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     null|                  30|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10004223,-,116...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2115|    20|3029659|  3029659|3029707|   0|     2H12M2D35M102H|   20|16006461|  49|TTTTTTTTTCTTTTTTT...|;=?>==<<=>?=====;...|  null|    34|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  133M18H|          12^TC11A23|  null|  null|  null|     3|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,16006671,-,81M...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...| 129|    20|3375591|  3375591|3375630|   0|          28S40M83S|   20|15982098|  40|AGAACTGAGGATTGAAC...|/;;0+0+-+6,:07==)...|  null|    35|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                34T5|  null|    60|  null|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,15982568,-,31S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2195|    20|3858310|  3858310|3858342|   0|          112H33M6H|   20|16054666|  33|TAAAAAAAAAAAAAAAA...|+=,1=+==,9<<;<<<+...|  null|    33|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  33|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,16054983,-,55S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2115|    20|4289927|  4289927|4289956|   4|          119H30M2H|   20|10180591|  30|TGATCTTGGCTTACTGC...|#################...|  null|    30|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                  30|  null|  null|  null|     0|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,10180887,-,80S...|  null|  null|  null|  null|\n",
      "| mother10|H0164ALXX140820:2...|2179|    20|4300760|  4300760|4300794|   0|            35M116H|   20|15800498|  35|CTATTTTTTTTTTTTTT...|/3./=,=,10++=<,+<...|  null|    30|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|     151M|                27T7|  null|  null|  null|     1|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|  null|    S1|  null|20,15800665,-,31S...|  null|  null|  null|  null|\n",
      "+---------+--------------------+----+------+-------+---------+-------+----+-------------------+-----+--------+----+--------------------+--------------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+---------+--------------------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+------+--------------------+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wybór kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+----+\n",
      "|sample_id|contig|    pos|mapq|\n",
      "+---------+------+-------+----+\n",
      "| mother10|    20|  83235|   0|\n",
      "| mother10|    20| 293889|   0|\n",
      "| mother10|    20| 465705|   7|\n",
      "| mother10|    20| 465710|   0|\n",
      "| mother10|    20| 723766|   0|\n",
      "| mother10|    20| 723766|   0|\n",
      "| mother10|    20| 723766|   6|\n",
      "| mother10|    20|1543878|   0|\n",
      "| mother10|    20|1648876|   0|\n",
      "| mother10|    20|2058293|   6|\n",
      "| mother10|    20|2522293|   0|\n",
      "| mother10|    20|2806380|  52|\n",
      "| mother10|    20|2831118|   0|\n",
      "| mother10|    20|2849265|   3|\n",
      "| mother10|    20|2989376|   0|\n",
      "| mother10|    20|3029659|   0|\n",
      "| mother10|    20|3375591|   0|\n",
      "| mother10|    20|3858310|   0|\n",
      "| mother10|    20|4289927|   4|\n",
      "| mother10|    20|4300760|   0|\n",
      "+---------+------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|flag|\n",
      "+----+\n",
      "|  65|\n",
      "|  69|\n",
      "|  73|\n",
      "|  81|\n",
      "|  83|\n",
      "|  97|\n",
      "|  99|\n",
      "| 113|\n",
      "| 117|\n",
      "| 121|\n",
      "| 129|\n",
      "| 133|\n",
      "| 137|\n",
      "| 145|\n",
      "| 147|\n",
      "| 161|\n",
      "| 163|\n",
      "| 177|\n",
      "| 181|\n",
      "| 185|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select distinct flag from {table_bam} order by flag ASC\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Czym (jakim typem danych) są dane zwracane przez spark.sql (\"SELECT..\")?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- flag: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(f\"select distinct flag from {table_bam} order by flag ASC\") \n",
    "print(type(df))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrowanie wierszy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+----+\n",
      "|sample_id|contig|    pos|mapq|\n",
      "+---------+------+-------+----+\n",
      "| mother10|    20|  83235|   0|\n",
      "| mother10|    20| 293889|   0|\n",
      "| mother10|    20| 465705|   7|\n",
      "| mother10|    20| 465710|   0|\n",
      "| mother10|    20| 723766|   0|\n",
      "| mother10|    20| 723766|   0|\n",
      "| mother10|    20| 723766|   6|\n",
      "| mother10|    20|1543878|   0|\n",
      "| mother10|    20|1648876|   0|\n",
      "| mother10|    20|2058293|   6|\n",
      "| mother10|    20|2522293|   0|\n",
      "| mother10|    20|2806380|  52|\n",
      "| mother10|    20|2831118|   0|\n",
      "| mother10|    20|2849265|   3|\n",
      "| mother10|    20|2989376|   0|\n",
      "| mother10|    20|3029659|   0|\n",
      "| mother10|    20|3375591|   0|\n",
      "+---------+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where pos < 3858310\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+----+\n",
      "|sample_id|contig|    pos|mapq|\n",
      "+---------+------+-------+----+\n",
      "| mother10|    20|2806380|  52|\n",
      "+---------+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where pos < 3858310 and flag NOT IN (113,117,121) and mapq > 50\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Napisz zapytanie które zwróci unikalne wartości CIGAR na contigu 20 na pozycjach 1-1000000. Ile jest takich unikalnych wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"select distinct cigar from {table_bam} where pos between 1 and 1000000 and contig= '20'\").count()\n",
    "spark.sql(f\"select count (distinct cigar) from {table_bam} where pos between 1 and 1000000 and contig= '20'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokaż ile jest wierszy o konkretnej wartości flagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|flag|  cnt|\n",
      "+----+-----+\n",
      "| 147|28618|\n",
      "|  99|28618|\n",
      "|  83|28508|\n",
      "| 163|28508|\n",
      "| 121|  248|\n",
      "| 181|  248|\n",
      "|  73|  220|\n",
      "| 133|  220|\n",
      "|  97|  114|\n",
      "| 161|  114|\n",
      "|  81|  114|\n",
      "| 145|  114|\n",
      "|2179|   47|\n",
      "|2227|   33|\n",
      "|2115|   33|\n",
      "|2211|   31|\n",
      "|2195|   30|\n",
      "|  65|   25|\n",
      "| 129|   25|\n",
      "|2163|   22|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select flag, count(*) as cnt from {table_bam} group by flag order by cnt desc\").show()  # AS - alias kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozna wykorzystac kilka funkcji agregujących (na roznych kolumnach) w jednym zapytaniu. \n",
    "\n",
    "Pokaz ile odczytow ma dana flage. Pokaz takze ich srednia jakosc mapowania oraz minimalna pozycje na ktorej wystepuja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------------------+--------------+\n",
      "|flag|cnt|             avg_m|min(pos_start)|\n",
      "+----+---+------------------+--------------+\n",
      "| 137|  1|              24.0|      15863674|\n",
      "|2225|  1|               1.0|      25535261|\n",
      "|2185|  1|               0.0|      33288211|\n",
      "|  69|  1|               0.0|      15863674|\n",
      "|2161|  3|               5.0|       2522293|\n",
      "|2177|  3|2.3333333333333335|      32815939|\n",
      "|2145|  4|              6.25|      10024268|\n",
      "| 113|  5|              48.0|      10111665|\n",
      "| 177|  5|              38.8|      10144245|\n",
      "| 185|  5|              25.8|      10101679|\n",
      "|2113|  5|              12.0|      32815939|\n",
      "|2121|  5|               8.2|       5448232|\n",
      "|2169|  5|               7.8|        293889|\n",
      "|2129|  5|               0.0|       8986284|\n",
      "| 117|  5|               0.0|      10101679|\n",
      "|2209|  8|             29.25|      10111387|\n",
      "|2131| 16|           16.5625|       2831118|\n",
      "|2193| 18| 16.72222222222222|       7416672|\n",
      "|2147| 22| 32.45454545454545|      10097473|\n",
      "|2163| 22| 8.954545454545455|       8986285|\n",
      "+----+---+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select flag, count(*) as cnt, avg(mapq) as avg_m, min(pos_start) as min from {table_bam} group by flag order by cnt, avg_m desc\").show()  # AS - alias kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcje mozemy zagniezdzac. Np: round(avg(kolumna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+--------------+\n",
      "|flag|cnt|avg_m|min(pos_start)|\n",
      "+----+---+-----+--------------+\n",
      "| 137|  1|   24|      15863674|\n",
      "|2225|  1|    1|      25535261|\n",
      "|2185|  1|    0|      33288211|\n",
      "|  69|  1|    0|      15863674|\n",
      "|2161|  3|    5|       2522293|\n",
      "|2177|  3|    2|      32815939|\n",
      "|2145|  4|    6|      10024268|\n",
      "| 113|  5|   48|      10111665|\n",
      "| 177|  5|   38|      10144245|\n",
      "| 185|  5|   25|      10101679|\n",
      "|2113|  5|   12|      32815939|\n",
      "|2121|  5|    8|       5448232|\n",
      "|2169|  5|    7|        293889|\n",
      "|2129|  5|    0|       8986284|\n",
      "| 117|  5|    0|      10101679|\n",
      "|2209|  8|   29|      10111387|\n",
      "|2131| 16|   16|       2831118|\n",
      "|2193| 18|   16|       7416672|\n",
      "|2147| 22|   32|      10097473|\n",
      "|2163| 22|    8|       8986285|\n",
      "+----+---+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uzycie funkcji floor na średniej\n",
    "spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min from {table_bam} group by flag order by cnt, avg_m desc\").show()  # AS - alias kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Wiedząc, że jako wyrażenie grupujące można wykorzystać wywołanie funkcji na kolumnie napisz zapytanie które policzy ile odczytów ma określoną długość CIGARa. Dla tych grup policz także zaokrągloną (ROUND) wartość jakości mapowania odczytów. Wynik zawęź tylko do odczytów występujących na chromosomie 20 lub 21. Posortuj malejaco liczności grup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------+\n",
      "|length(cigar)|  cnt|avg_map|\n",
      "+-------------+-----+-------+\n",
      "|            4|93007|   60.0|\n",
      "|            7|16142|   60.0|\n",
      "|            6| 2540|   59.0|\n",
      "|            9| 1866|   53.0|\n",
      "|            8|  833|   58.0|\n",
      "|           10|  341|   55.0|\n",
      "|           11|  323|   59.0|\n",
      "|           12|  216|   59.0|\n",
      "|           13|  152|   59.0|\n",
      "|           15|   30|   59.0|\n",
      "|           14|   30|   39.0|\n",
      "|           16|   27|   59.0|\n",
      "|           17|   11|   58.0|\n",
      "|           21|    5|   60.0|\n",
      "|           18|    4|   36.0|\n",
      "|           20|    3|   57.0|\n",
      "|           26|    1|    3.0|\n",
      "|           19|    1|    0.0|\n",
      "|           22|    1|   60.0|\n",
      "|           24|    1|    0.0|\n",
      "+-------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select length(cigar), count(*) as cnt, round (avg(mapq)) as avg_map \\\n",
    "            from {table_bam} \\\n",
    "            where contig IN ('20', '21') \\\n",
    "            group by length(cigar) \\\n",
    "            order by cnt desc\").show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrowanie grup danych\n",
    "\n",
    "Warunki na pojedyncze wiersze nakładamy za pomocą klauzuli WHERE. Jeśli dokonujemy grupowania danych i chcemy w wynikach uzyskać jedynie informacje o grupach, które spełniają określone warunki używamy klauzuli HAVING. (HAVING moze wystapic wylacznie z GROUP BY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------+\n",
      "|flag|  cnt|avg_m|     min|\n",
      "+----+-----+-----+--------+\n",
      "| 163|28508|   59| 9999896|\n",
      "|  83|28508|   59|10000096|\n",
      "| 147|28618|   59| 9998345|\n",
      "|  99|28618|   59| 9998296|\n",
      "+----+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zrob podsumowanie dla grup odczytów o zgodnym flag. Policz srednie mapq oraz minmalna pozycje\n",
    "# Pokaz jedynie informacja o flagach dla ktorych jest ponad 1000 odczytów.\n",
    "spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min \\\n",
    "            from {table_bam} \\\n",
    "            group by flag \\\n",
    "            having count(*) > 1000 \\\n",
    "            order by cnt, avg_m desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Czy w klauzuli HAVING możemy użyc aliasu kolumny cnt?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warunki w HAVING też mogą być złożone (złączone AND, OR). Ale warunki w HAVING mogą się odnosić jedynie do kolumn grupujących lub funkcji agregujących"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------+\n",
      "|flag|  cnt|avg_m|     min|\n",
      "+----+-----+-----+--------+\n",
      "|  83|28508|   59|10000096|\n",
      "| 163|28508|   59| 9999896|\n",
      "| 147|28618|   59| 9998345|\n",
      "+----+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min \\\n",
    "            from {table_bam} \\\n",
    "            group by flag \\\n",
    "            having count(*) > 1000 and flag != 99 \\\n",
    "            order by cnt, avg_m desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bledna konstrukcja - W HAVING mamy warunek na kolumny niegrupujace\n",
    "# spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min \\\n",
    "#             from {table_bam} \\\n",
    "#             group by flag \\\n",
    "#             having count(*) > 1000 and sample_id ='mother' \\\n",
    "#             order by cnt, avg_m desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Wiedząc, że jako wyrażenie grupujące można wykorzystać wywołanie funkcji na kolumnie napisz zapytanie które policzy ile odczytów ma określoną długość CIGARa. \n",
    "Dla tych grup policz także zaokrągloną (ROUND) wartość jakości mapowania odczytów. \n",
    "Wynik zawęź tylko do odczytów występujących na chromosomie 20 lub 21. Pokaż tylko grupy mające średnia jakosc mapowania powyzej 30. Posortuj malejaco liczności grup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------+\n",
      "|length(cigar)|  cnt|avg_map|\n",
      "+-------------+-----+-------+\n",
      "|            4|93007|   60.0|\n",
      "|            7|16142|   60.0|\n",
      "|            6| 2540|   59.0|\n",
      "|            9| 1866|   53.0|\n",
      "|            8|  833|   58.0|\n",
      "|           10|  341|   55.0|\n",
      "|           11|  323|   59.0|\n",
      "|           12|  216|   59.0|\n",
      "|           13|  152|   59.0|\n",
      "|           15|   30|   59.0|\n",
      "|           14|   30|   39.0|\n",
      "|           16|   27|   59.0|\n",
      "|           17|   11|   58.0|\n",
      "|           21|    5|   60.0|\n",
      "|           18|    4|   36.0|\n",
      "|           20|    3|   57.0|\n",
      "|           22|    1|   60.0|\n",
      "+-------------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select length(cigar), count(*) as cnt, round (avg(mapq)) as avg_map \\\n",
    "            from {table_bam} \\\n",
    "            where contig IN ('20', '21') \\\n",
    "            group by length(cigar) \\\n",
    "            having avg(mapq) > 30 \\\n",
    "            order by cnt desc\").show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie funkcji wbudowanych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niektóre funkcje do wykorzystania:\n",
    "ROUND, FLOOR, SIGN, POW, LEAST, LOG\n",
    "UPPER, LOWER, SUBSTR, \n",
    "NVL\n",
    "CURRENT_DATE\n",
    "MIN, MAX, SUM, STDDEV, AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------+\n",
      "|sample_id|upper(sample_id)|current_date()|\n",
      "+---------+----------------+--------------+\n",
      "| mother10|        MOTHER10|    2021-01-15|\n",
      "+---------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select distinct sample_id, upper(sample_id), current_date() from {table_bam} \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie funkcji rozszerzonych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Istnieją rozszerzenia Sparka do obslugi genomicznych danych. Na przykład pakiet sequila dostarcza metod do rozproszonego wyliczenia glebokosci pokrycia i pileup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeby skorzystac z rozszerzen importujemy modul i tworzymy obiekt ss, opakowanie na sesje sparkową\n",
    "from pysequila import SequilaSession\n",
    "ss = SequilaSession(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage jest dostępny jako funkcja tabelaryczna, czyli funkcja zwracająca tabelę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konstruujemy zapytanie\n",
    "cov_query = f\"select * from coverage ('{table_bam}', 'mother10', 'blocks')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+--------+\n",
      "|contig|pos_start|pos_end|coverage|\n",
      "+------+---------+-------+--------+\n",
      "|    20|    83235|  83303|       1|\n",
      "|    20|    83304| 293920|       1|\n",
      "|    20|   293921| 465709|       1|\n",
      "|    20|   465710| 465753|       2|\n",
      "|    20|   465754| 465755|       1|\n",
      "|    20|   465756| 723832|       3|\n",
      "|    20|   723833| 723835|       1|\n",
      "|    20|   723836|1543985|       1|\n",
      "|    20|  1543986|1648918|       1|\n",
      "|    20|  1648919|1648928|       1|\n",
      "|    20|  1648929|2058323|       1|\n",
      "|    20|  2058324|2522335|       1|\n",
      "|    20|  2522336|2806409|       1|\n",
      "|    20|  2806410|2831171|       1|\n",
      "|    20|  2831172|2849299|       1|\n",
      "|    20|  2849300|2989405|       1|\n",
      "|    20|  2989406|3029670|       1|\n",
      "|    20|  3029671|3029707|       1|\n",
      "|    20|  3029708|3375630|       1|\n",
      "|    20|  3375631|3858342|       1|\n",
      "+------+---------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(cov_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Pokaż tylko pozycje, które mają głębokość pokrycia większą niż 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+\n",
      "|contig|pos_start| pos_end|coverage|\n",
      "+------+---------+--------+--------+\n",
      "|    20|  4553753| 4553753|       5|\n",
      "|    20|  4553754| 4553795|       6|\n",
      "|    20|  4553796| 4553801|       5|\n",
      "|    20|  8986286| 8986330|       5|\n",
      "|    20|  9999909| 9999929|       5|\n",
      "|    20|  9999930| 9999944|       6|\n",
      "|    20|  9999945| 9999975|       7|\n",
      "|    20|  9999976| 9999978|       8|\n",
      "|    20|  9999979| 9999981|       9|\n",
      "|    20|  9999982| 9999986|      10|\n",
      "|    20|  9999987| 9999995|      12|\n",
      "|    20|  9999996| 9999996|      13|\n",
      "|    20|  9999997| 9999998|      15|\n",
      "|    20|  9999999|10000022|      16|\n",
      "|    20| 10000023|10000026|      17|\n",
      "|    20| 10000027|10000027|      18|\n",
      "|    20| 10000028|10000036|      17|\n",
      "|    20| 10000037|10000044|      18|\n",
      "|    20| 10000045|10000053|      16|\n",
      "|    20| 10000054|10000054|      15|\n",
      "+------+---------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(f\"select * from coverage ('{table_bam}', 'mother10', 'blocks') where coverage >= 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Korzystając z opcji uruchomienia funkcji coverage ktora zwraca pokrycie dla kazdej pozycji niezaleznie (bases zamiast blocks) napisz  zapytanie które zwróci ile pozycji ma daną głębokość pokrycia. Posortuj po liczności malejąco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|coverage|  cnt|\n",
      "+--------+-----+\n",
      "|      33|31078|\n",
      "|      34|30834|\n",
      "|      32|30473|\n",
      "|      35|30019|\n",
      "|      31|29246|\n",
      "|      36|28492|\n",
      "|      30|26587|\n",
      "|      37|26499|\n",
      "|      38|23766|\n",
      "|      29|23373|\n",
      "|      39|21490|\n",
      "|      28|20130|\n",
      "|      40|19316|\n",
      "|      27|17490|\n",
      "|      41|16458|\n",
      "|      26|14431|\n",
      "|      42|13556|\n",
      "|      25|11710|\n",
      "|      43|11226|\n",
      "|      24| 9320|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(f\"select coverage, count(*) as cnt from coverage ('{table_bam}', 'mother10', 'bases') group by coverage order by cnt desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykrywanie wariantów\n",
    "Na zajeciach skorzystamy z wersji rozproszonej HaplotypeCaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Running GATK with users' params: --conf spark.executor.memory=1g --conf spark.driver.memory=1g --conf spark.executor.instances=2 --conf spark.hadoop.fs.gs.block.size=8388608 -R /mnt/data/mapping/ref/ref.fasta -I gs://edugen-lab-agaszmurlo/bam/mother10.bam -O gs://edugen-lab-agaszmurlo/vcf/mother10.vcf on Kubernetes\n",
      "Using GATK jar /opt/gatk-4.1.9.0/gatk-package-4.1.9.0-spark.jar\n",
      "Running:\n",
      "    /usr/local/spark/bin/spark-submit --master k8s://https://10.3.240.1:443 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2  --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2  /opt/gatk-4.1.9.0/gatk-package-4.1.9.0-spark.jar HaplotypeCallerSpark --spark-master k8s://https://10.3.240.1:443 --conf spark.jars=/tmp/gcs-connector-hadoop2-1.9.17-shaded.jar,/tmp/google-cloud-nio-0.120.0-alpha-shaded.jar --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/tmp/secrets/ds-lab-sa.json --conf spark.kubernetes.driverEnv.GCS_PROJECT_ID=studiapodyplomowe --conf spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS=/tmp/secrets/ds-lab-sa.json --conf spark.executorEnv.GCS_PROJECT_ID=studiapodyplomowe --conf spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS=/tmp/secrets/ds-lab-sa.json --conf spark.kubernetes.driver.secrets.ds-lab-sa-secret=/tmp/secrets --conf spark.kubernetes.executor.secrets.ds-lab-sa-secret=/tmp/secrets --conf spark.hadoop.fs.gs.project.id=studiapodyplomowe --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS --conf spark.kubernetes.container.image=biodatageeks/spark-py:v2.4.3-edugen-0.1.7-gatk --conf spark.kubernetes.authenticate.driver.serviceAccountName=ds-lab-sa --conf spark.kubernetes.authenticate.serviceAccountName=ds-lab-sa --conf spark.kubernetes.executor.podNamePrefix=gatk-exec-agaszmurlo --conf spark.kubernetes.executor.label.spark-owner=agaszmurlo --conf spark.kubernetes.executor.request.cores=0.4 --conf spark.driver.port=29010 --conf spark.blockManager.port=29011 --conf spark.kubernetes.namespace=default --conf spark.driver.host=jupyter-service-agaszmurlo --conf spark.driver.bindAddress=jupyter-agaszmurlo --conf spark.executorEnv.PYSPARK_PYTHON=python3 --conf spark.executor.memory=1g --conf spark.driver.memory=1g --conf spark.executor.instances=2 --conf spark.hadoop.fs.gs.block.size=8388608 -R /mnt/data/mapping/ref/ref.fasta -I gs://edugen-lab-agaszmurlo/bam/mother10.bam -O gs://edugen-lab-agaszmurlo/vcf/mother10.vcf\n",
      "21/01/15 14:34:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "14:34:27.936 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/opt/gatk-4.1.9.0/gatk-package-4.1.9.0-spark.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "14:34:29.425 INFO  HaplotypeCallerSpark - ------------------------------------------------------------\n",
      "14:34:29.426 INFO  HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.9.0\n",
      "14:34:29.426 INFO  HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "14:34:29.427 INFO  HaplotypeCallerSpark - Executing as jovyan@jupyter-agaszmurlo on Linux v4.19.112+ amd64\n",
      "14:34:29.427 INFO  HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_275-b01\n",
      "14:34:29.427 INFO  HaplotypeCallerSpark - Start Date/Time: January 15, 2021 2:34:27 PM UTC\n",
      "14:34:29.427 INFO  HaplotypeCallerSpark - ------------------------------------------------------------\n",
      "14:34:29.427 INFO  HaplotypeCallerSpark - ------------------------------------------------------------\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - HTSJDK Version: 2.23.0\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - Picard Version: 2.23.3\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "14:34:29.429 INFO  HaplotypeCallerSpark - Deflater: IntelDeflater\n",
      "14:34:29.430 INFO  HaplotypeCallerSpark - Inflater: IntelInflater\n",
      "14:34:29.430 INFO  HaplotypeCallerSpark - GCS max retries/reopens: 20\n",
      "14:34:29.430 INFO  HaplotypeCallerSpark - Requester pays: disabled\n",
      "14:34:29.430 WARN  HaplotypeCallerSpark - \n",
      "\n",
      "\u001b[1m\u001b[31m   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "   Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production\n",
      "\n",
      "   !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "\n",
      "\n",
      "14:34:29.430 INFO  HaplotypeCallerSpark - Initializing engine\n",
      "14:34:29.430 INFO  HaplotypeCallerSpark - Done initializing engine\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/01/15 14:34:29 INFO SparkContext: Running Spark version 2.4.3\n",
      "21/01/15 14:34:29 INFO SparkContext: Submitted application: HaplotypeCallerSpark\n",
      "21/01/15 14:34:29 INFO SecurityManager: Changing view acls to: jovyan\n",
      "21/01/15 14:34:29 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "21/01/15 14:34:29 INFO SecurityManager: Changing view acls groups to: \n",
      "21/01/15 14:34:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/01/15 14:34:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "21/01/15 14:34:30 INFO Utils: Successfully started service 'sparkDriver' on port 29010.\n",
      "21/01/15 14:34:30 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/01/15 14:34:30 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/01/15 14:34:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/01/15 14:34:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/01/15 14:34:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1506edfb-cb35-42ec-93a5-27517b690858\n",
      "21/01/15 14:34:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "21/01/15 14:34:30 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/01/15 14:34:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/01/15 14:34:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://jupyter-service-agaszmurlo:4040\n",
      "21/01/15 14:34:31 INFO SparkContext: Added JAR /tmp/gcs-connector-hadoop2-1.9.17-shaded.jar at spark://jupyter-service-agaszmurlo:29010/jars/gcs-connector-hadoop2-1.9.17-shaded.jar with timestamp 1610721271607\n",
      "21/01/15 14:34:31 INFO SparkContext: Added JAR /tmp/google-cloud-nio-0.120.0-alpha-shaded.jar at spark://jupyter-service-agaszmurlo:29010/jars/google-cloud-nio-0.120.0-alpha-shaded.jar with timestamp 1610721271610\n",
      "21/01/15 14:34:33 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes.\n",
      "21/01/15 14:34:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29011.\n",
      "21/01/15 14:34:34 INFO NettyBlockTransferService: Server created on jupyter-service-agaszmurlo:29011\n",
      "21/01/15 14:34:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/01/15 14:34:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, jupyter-service-agaszmurlo, 29011, None)\n",
      "21/01/15 14:34:34 INFO BlockManagerMasterEndpoint: Registering block manager jupyter-service-agaszmurlo:29011 with 366.3 MB RAM, BlockManagerId(driver, jupyter-service-agaszmurlo, 29011, None)\n",
      "21/01/15 14:34:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, jupyter-service-agaszmurlo, 29011, None)\n",
      "21/01/15 14:34:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, jupyter-service-agaszmurlo, 29011, None)\n",
      "21/01/15 14:34:41 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.12.107:52556) with ID 1\n",
      "21/01/15 14:34:41 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.12.108:32962) with ID 2\n",
      "21/01/15 14:34:41 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "14:34:41.988 INFO  HaplotypeCallerSpark - Spark verbosity set to INFO (see --spark-verbosity argument)\n",
      "21/01/15 14:34:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.12.108:29011 with 413.9 MB RAM, BlockManagerId(2, 10.0.12.108, 29011, None)\n",
      "21/01/15 14:34:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.12.107:29011 with 413.9 MB RAM, BlockManagerId(1, 10.0.12.107, 29011, None)\n",
      "14:34:45.203 INFO  HaplotypeCallerSpark - ********************************************************************************\n",
      "14:34:45.203 INFO  HaplotypeCallerSpark - The output of this tool DOES NOT match the output of HaplotypeCaller. \n",
      "14:34:45.203 INFO  HaplotypeCallerSpark - It is under development and should not be used for production work. \n",
      "14:34:45.203 INFO  HaplotypeCallerSpark - For evaluation only.\n",
      "14:34:45.204 INFO  HaplotypeCallerSpark - Use the non-spark HaplotypeCaller if you care about the results. \n",
      "14:34:45.204 INFO  HaplotypeCallerSpark - ********************************************************************************\n",
      "21/01/15 14:34:45 INFO GoogleHadoopFileSystemBase: GHFS version: 1.9.4-hadoop3\n",
      "21/01/15 14:34:45 INFO SparkContext: Added file file:///mnt/data/mapping/ref/ref.fasta at spark://jupyter-service-agaszmurlo:29010/files/ref.fasta with timestamp 1610721285833\n",
      "21/01/15 14:34:45 INFO Utils: Copying /mnt/data/mapping/ref/ref.fasta to /tmp/spark-a7712177-2991-494b-bf43-791f17e6f7ba/userFiles-4a52a5eb-a3b5-475a-9097-a67844d370a4/ref.fasta\n",
      "21/01/15 14:34:46 INFO SparkContext: Added file file:///mnt/data/mapping/ref/ref.fasta.fai at spark://jupyter-service-agaszmurlo:29010/files/ref.fasta.fai with timestamp 1610721286564\n",
      "21/01/15 14:34:46 INFO Utils: Copying /mnt/data/mapping/ref/ref.fasta.fai to /tmp/spark-a7712177-2991-494b-bf43-791f17e6f7ba/userFiles-4a52a5eb-a3b5-475a-9097-a67844d370a4/ref.fasta.fai\n",
      "21/01/15 14:34:46 INFO SparkContext: Added file file:///mnt/data/mapping/ref/ref.dict at spark://jupyter-service-agaszmurlo:29010/files/ref.dict with timestamp 1610721286577\n",
      "21/01/15 14:34:46 INFO Utils: Copying /mnt/data/mapping/ref/ref.dict to /tmp/spark-a7712177-2991-494b-bf43-791f17e6f7ba/userFiles-4a52a5eb-a3b5-475a-9097-a67844d370a4/ref.dict\n",
      "21/01/15 14:34:46 WARN GoogleHadoopFileSystemBase: No working directory configured, using default: 'gs://edugen-lab-agaszmurlo/'\n",
      "21/01/15 14:34:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 224.0 B, free 366.3 MB)\n",
      "21/01/15 14:34:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 123.0 B, free 366.3 MB)\n",
      "21/01/15 14:34:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 123.0 B, free: 366.3 MB)\n",
      "21/01/15 14:34:50 INFO SparkContext: Created broadcast 0 from broadcast at AbstractBinarySamSource.java:82\n",
      "21/01/15 14:34:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 416.0 B, free 366.3 MB)\n",
      "21/01/15 14:34:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 255.0 B, free 366.3 MB)\n",
      "21/01/15 14:34:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 255.0 B, free: 366.3 MB)\n",
      "21/01/15 14:34:51 INFO SparkContext: Created broadcast 1 from broadcast at BamSource.java:104\n",
      "21/01/15 14:34:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 279.8 KB, free 366.0 MB)\n",
      "21/01/15 14:34:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 30.1 KB, free 366.0 MB)\n",
      "21/01/15 14:34:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 30.1 KB, free: 366.3 MB)\n",
      "21/01/15 14:34:52 INFO SparkContext: Created broadcast 2 from newAPIHadoopFile at PathSplitSource.java:96\n",
      "21/01/15 14:34:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 37.1 MB, free 328.9 MB)\n",
      "21/01/15 14:34:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2037.0 B, free 328.9 MB)\n",
      "21/01/15 14:34:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 2037.0 B, free: 366.3 MB)\n",
      "21/01/15 14:34:52 INFO SparkContext: Created broadcast 3 from broadcast at HaplotypeCallerSpark.java:236\n",
      "21/01/15 14:34:52 INFO FileInputFormat: Total input paths to process : 1\n",
      "21/01/15 14:34:53 INFO SparkContext: Starting job: collect at SparkSharder.java:388\n",
      "21/01/15 14:34:53 INFO DAGScheduler: Got job 0 (collect at SparkSharder.java:388) with 2 output partitions\n",
      "21/01/15 14:34:53 INFO DAGScheduler: Final stage: ResultStage 0 (collect at SparkSharder.java:388)\n",
      "21/01/15 14:34:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/01/15 14:34:53 INFO DAGScheduler: Missing parents: List()\n",
      "21/01/15 14:34:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at mapPartitions at SparkSharder.java:386), which has no missing parents\n",
      "21/01/15 14:34:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 81.6 KB, free 328.8 MB)\n",
      "21/01/15 14:34:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 37.0 KB, free 328.7 MB)\n",
      "21/01/15 14:34:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 37.0 KB, free: 366.2 MB)\n",
      "21/01/15 14:34:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161\n",
      "21/01/15 14:34:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at mapPartitions at SparkSharder.java:386) (first 15 tasks are for partitions Vector(0, 1))\n",
      "21/01/15 14:34:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "21/01/15 14:34:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.12.108, executor 2, partition 0, PROCESS_LOCAL, 7957 bytes)\n",
      "21/01/15 14:34:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.0.12.107, executor 1, partition 1, PROCESS_LOCAL, 7957 bytes)\n",
      "21/01/15 14:34:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.12.107:29011 (size: 37.0 KB, free: 413.9 MB)\n",
      "21/01/15 14:34:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.12.108:29011 (size: 37.0 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.12.107:29011 (size: 30.1 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.12.108:29011 (size: 30.1 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.12.107:29011 (size: 255.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.12.108:29011 (size: 255.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.12.108:29011 (size: 123.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.12.107:29011 (size: 123.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 12988 ms on 10.0.12.108 (executor 2) (1/2)\n",
      "21/01/15 14:35:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 13012 ms on 10.0.12.107 (executor 1) (2/2)\n",
      "21/01/15 14:35:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/01/15 14:35:06 INFO DAGScheduler: ResultStage 0 (collect at SparkSharder.java:388) finished in 13.274 s\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Job 0 finished: collect at SparkSharder.java:388, took 13.418681 s\n",
      "21/01/15 14:35:06 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 200.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:06 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 84.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:06 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 84.0 B, free: 366.2 MB)\n",
      "21/01/15 14:35:06 INFO SparkContext: Created broadcast 5 from broadcast at SparkSharder.java:214\n",
      "21/01/15 14:35:06 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 504.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:06 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 282.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:06 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 282.0 B, free: 366.2 MB)\n",
      "21/01/15 14:35:06 INFO SparkContext: Created broadcast 6 from broadcast at SparkSharder.java:221\n",
      "21/01/15 14:35:06 INFO SparkContext: Starting job: collectAsMap at SparkSharder.java:258\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Registering RDD 10 (mapToPair at SparkSharder.java:247)\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Registering RDD 13 (mapToPair at SparkSharder.java:255)\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Got job 1 (collectAsMap at SparkSharder.java:258) with 2 output partitions\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Final stage: ResultStage 3 (collectAsMap at SparkSharder.java:258)\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[10] at mapToPair at SparkSharder.java:247), which has no missing parents\n",
      "21/01/15 14:35:06 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 6.3 KB, free 328.7 MB)\n",
      "21/01/15 14:35:06 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.8 KB, free 328.7 MB)\n",
      "21/01/15 14:35:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 3.8 KB, free: 366.2 MB)\n",
      "21/01/15 14:35:06 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161\n",
      "21/01/15 14:35:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[10] at mapToPair at SparkSharder.java:247) (first 15 tasks are for partitions Vector(0, 1))\n",
      "21/01/15 14:35:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n",
      "21/01/15 14:35:06 WARN TaskSetManager: Stage 1 contains a task of very large size (155 KB). The maximum recommended task size is 100 KB.\n",
      "21/01/15 14:35:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 10.0.12.108, executor 2, partition 0, PROCESS_LOCAL, 158934 bytes)\n",
      "21/01/15 14:35:06 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 10.0.12.107, executor 1, partition 1, PROCESS_LOCAL, 159785 bytes)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.12.107:29011 (size: 3.8 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.12.108:29011 (size: 3.8 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.12.107:29011 (size: 282.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.12.108:29011 (size: 282.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.12.108:29011 (size: 84.0 B, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 760 ms on 10.0.12.107 (executor 1) (1/2)\n",
      "21/01/15 14:35:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 900 ms on 10.0.12.108 (executor 2) (2/2)\n",
      "21/01/15 14:35:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/01/15 14:35:07 INFO DAGScheduler: ShuffleMapStage 1 (mapToPair at SparkSharder.java:247) finished in 0.943 s\n",
      "21/01/15 14:35:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/01/15 14:35:07 INFO DAGScheduler: running: Set()\n",
      "21/01/15 14:35:07 INFO DAGScheduler: waiting: Set(ShuffleMapStage 2, ResultStage 3)\n",
      "21/01/15 14:35:07 INFO DAGScheduler: failed: Set()\n",
      "21/01/15 14:35:07 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[13] at mapToPair at SparkSharder.java:255), which has no missing parents\n",
      "21/01/15 14:35:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 6.1 KB, free 328.7 MB)\n",
      "21/01/15 14:35:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.7 KB, free 328.7 MB)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 3.7 KB, free: 366.2 MB)\n",
      "21/01/15 14:35:07 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161\n",
      "21/01/15 14:35:07 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[13] at mapToPair at SparkSharder.java:255) (first 15 tasks are for partitions Vector(0, 1))\n",
      "21/01/15 14:35:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks\n",
      "21/01/15 14:35:07 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, 10.0.12.107, executor 1, partition 1, NODE_LOCAL, 7663 bytes)\n",
      "21/01/15 14:35:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5, 10.0.12.108, executor 2, partition 0, NODE_LOCAL, 7663 bytes)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.12.108:29011 (size: 3.7 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.12.107:29011 (size: 3.7 KB, free: 413.9 MB)\n",
      "21/01/15 14:35:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.12.108:32962\n",
      "21/01/15 14:35:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.12.107:52556\n",
      "21/01/15 14:35:08 INFO BlockManagerInfo: Added rdd_12_1 in memory on 10.0.12.107:29011 (size: 1358.7 KB, free: 412.5 MB)\n",
      "21/01/15 14:35:09 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 1505 ms on 10.0.12.107 (executor 1) (1/2)\n",
      "21/01/15 14:35:09 INFO BlockManagerInfo: Added rdd_12_0 in memory on 10.0.12.108:29011 (size: 448.4 KB, free: 413.4 MB)\n",
      "21/01/15 14:35:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 1589 ms on 10.0.12.108 (executor 2) (2/2)\n",
      "21/01/15 14:35:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21/01/15 14:35:09 INFO DAGScheduler: ShuffleMapStage 2 (mapToPair at SparkSharder.java:255) finished in 1.654 s\n",
      "21/01/15 14:35:09 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/01/15 14:35:09 INFO DAGScheduler: running: Set()\n",
      "21/01/15 14:35:09 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
      "21/01/15 14:35:09 INFO DAGScheduler: failed: Set()\n",
      "21/01/15 14:35:09 INFO DAGScheduler: Submitting ResultStage 3 (ShuffledRDD[14] at reduceByKey at SparkSharder.java:257), which has no missing parents\n",
      "21/01/15 14:35:09 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 4.2 KB, free 328.7 MB)\n",
      "21/01/15 14:35:09 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.4 KB, free 328.7 MB)\n",
      "21/01/15 14:35:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 2.4 KB, free: 366.2 MB)\n",
      "21/01/15 14:35:09 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161\n",
      "21/01/15 14:35:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (ShuffledRDD[14] at reduceByKey at SparkSharder.java:257) (first 15 tasks are for partitions Vector(0, 1))\n",
      "21/01/15 14:35:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks\n",
      "21/01/15 14:35:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 6, 10.0.12.108, executor 2, partition 0, NODE_LOCAL, 7674 bytes)\n",
      "21/01/15 14:35:09 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 7, 10.0.12.107, executor 1, partition 1, NODE_LOCAL, 7674 bytes)\n",
      "21/01/15 14:35:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.12.108:29011 (size: 2.4 KB, free: 413.4 MB)\n",
      "21/01/15 14:35:09 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.12.107:29011 (size: 2.4 KB, free: 412.5 MB)\n",
      "21/01/15 14:35:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.12.108:32962\n",
      "21/01/15 14:35:09 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.12.107:52556\n",
      "21/01/15 14:35:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 6) in 187 ms on 10.0.12.108 (executor 2) (1/2)\n",
      "21/01/15 14:35:09 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 7) in 236 ms on 10.0.12.107 (executor 1) (2/2)\n",
      "21/01/15 14:35:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "21/01/15 14:35:09 INFO DAGScheduler: ResultStage 3 (collectAsMap at SparkSharder.java:258) finished in 0.278 s\n",
      "21/01/15 14:35:09 INFO DAGScheduler: Job 1 finished: collectAsMap at SparkSharder.java:258, took 2.969370 s\n",
      "14:35:09.591 INFO  HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output\n",
      "14:35:09.622 INFO  NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/opt/gatk-4.1.9.0/gatk-package-4.1.9.0-spark.jar!/com/intel/gkl/native/libgkl_utils.so\n",
      "14:35:09.715 INFO  NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/opt/gatk-4.1.9.0/gatk-package-4.1.9.0-spark.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so\n",
      "14:35:09.768 INFO  IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM\n",
      "14:35:09.769 INFO  IntelPairHmm - Available threads: 4\n",
      "14:35:09.769 INFO  IntelPairHmm - Requested threads: 4\n",
      "14:35:09.770 INFO  PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation\n",
      "21/01/15 14:35:09 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 2.4 KB, free 328.7 MB)\n",
      "21/01/15 14:35:09 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 443.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:09 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 443.0 B, free: 366.2 MB)\n",
      "21/01/15 14:35:09 INFO SparkContext: Created broadcast 10 from broadcast at HaplotypeCallerSpark.java:151\n",
      "21/01/15 14:35:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 80.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 106.0 B, free 328.7 MB)\n",
      "21/01/15 14:35:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 106.0 B, free: 366.2 MB)\n",
      "21/01/15 14:35:09 INFO SparkContext: Created broadcast 11 from broadcast at HaplotypeCallerSpark.java:152\n",
      "21/01/15 14:35:10 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 72.7 MB, free 256.0 MB)\n",
      "21/01/15 14:35:10 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 374.0 B, free 256.0 MB)\n",
      "21/01/15 14:35:10 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 374.0 B, free: 366.2 MB)\n",
      "21/01/15 14:35:10 INFO SparkContext: Created broadcast 12 from broadcast at HaplotypeCallerSpark.java:153\n",
      "21/01/15 14:35:10 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 10.2 KB, free 256.0 MB)\n",
      "21/01/15 14:35:10 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1287.0 B, free 256.0 MB)\n",
      "21/01/15 14:35:10 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 1287.0 B, free: 366.2 MB)\n",
      "21/01/15 14:35:10 INFO SparkContext: Created broadcast 13 from broadcast at VcfSink.java:65\n",
      "21/01/15 14:35:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "21/01/15 14:35:12 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\n",
      "21/01/15 14:35:12 INFO DAGScheduler: Got job 2 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\n",
      "21/01/15 14:35:12 INFO DAGScheduler: Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)\n",
      "21/01/15 14:35:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "21/01/15 14:35:12 INFO DAGScheduler: Missing parents: List()\n",
      "21/01/15 14:35:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at mapToPair at VcfSink.java:77), which has no missing parents\n",
      "21/01/15 14:35:12 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 157.9 KB, free 255.8 MB)\n",
      "21/01/15 14:35:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 71.5 KB, free 255.7 MB)\n",
      "21/01/15 14:35:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on jupyter-service-agaszmurlo:29011 (size: 71.5 KB, free: 366.2 MB)\n",
      "21/01/15 14:35:12 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1161\n",
      "21/01/15 14:35:12 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at mapToPair at VcfSink.java:77) (first 15 tasks are for partitions Vector(0, 1))\n",
      "21/01/15 14:35:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks\n",
      "21/01/15 14:35:12 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8, 10.0.12.107, executor 1, partition 1, PROCESS_LOCAL, 8599 bytes)\n",
      "21/01/15 14:35:12 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9, 10.0.12.108, executor 2, partition 0, PROCESS_LOCAL, 8784 bytes)\n",
      "21/01/15 14:35:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.12.107:29011 (size: 71.5 KB, free: 412.5 MB)\n",
      "21/01/15 14:35:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.12.108:29011 (size: 71.5 KB, free: 413.3 MB)\n",
      "21/01/15 14:35:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.12.108:29011 (size: 2037.0 B, free: 413.3 MB)\n",
      "21/01/15 14:35:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.12.107:29011 (size: 2037.0 B, free: 412.5 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.12.108:29011 (size: 443.0 B, free: 413.3 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.12.107:29011 (size: 443.0 B, free: 412.5 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.12.107:29011 (size: 106.0 B, free: 412.5 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.12.108:29011 (size: 106.0 B, free: 413.3 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.12.107:29011 (size: 374.0 B, free: 412.4 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.12.108:29011 (size: 374.0 B, free: 413.3 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.12.107:29011 (size: 1287.0 B, free: 412.4 MB)\n",
      "21/01/15 14:35:13 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.12.108:29011 (size: 1287.0 B, free: 413.3 MB)\n",
      "21/01/15 14:36:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 53803 ms on 10.0.12.108 (executor 2) (1/2)\n",
      "21/01/15 14:36:11 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 59795 ms on 10.0.12.107 (executor 1) (2/2)\n",
      "21/01/15 14:36:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "21/01/15 14:36:11 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 59.842 s\n",
      "21/01/15 14:36:11 INFO DAGScheduler: Job 2 finished: runJob at SparkHadoopWriter.scala:78, took 59.851014 s\n",
      "21/01/15 14:36:20 INFO SparkHadoopWriter: Job job_20210115143510_0021 committed.\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 23\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 113\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 45\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 66\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 74\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 51\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 9\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 41\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 35\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 62\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 39\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 109\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 79\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 87\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 21\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 12\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 22\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 13\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 116\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 30\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 121\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 72\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 36\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 10\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 99\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 90\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 119\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on jupyter-service-agaszmurlo:29011 in memory (size: 37.0 KB, free: 366.2 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.12.108:29011 in memory (size: 37.0 KB, free: 413.4 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.12.107:29011 in memory (size: 37.0 KB, free: 412.5 MB)\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 91\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 25\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 75\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 42\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 83\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 117\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 124\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 11\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 28\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 31\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 102\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 38\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 59\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 8\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 57\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 94\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 3\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 70\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 95\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 120\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 85\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 114\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 55\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 92\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 6\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 16\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 24\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 115\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 14\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 81\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 104\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 20\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 110\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 60\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 34\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_8_piece0 on jupyter-service-agaszmurlo:29011 in memory (size: 3.7 KB, free: 366.2 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.12.107:29011 in memory (size: 3.7 KB, free: 412.5 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.12.108:29011 in memory (size: 3.7 KB, free: 413.4 MB)\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 26\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned shuffle 0\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 44\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 18\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 118\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 15\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 108\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 103\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 4\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 93\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 111\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 101\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 2\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 19\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 33\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 37\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 49\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 82\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 105\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 112\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 123\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 47\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 89\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 52\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 5\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 32\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 61\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 64\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 73\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 54\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 122\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 50\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 1\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 78\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 7\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 71\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 97\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 46\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 88\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on jupyter-service-agaszmurlo:29011 in memory (size: 3.8 KB, free: 366.2 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.12.107:29011 in memory (size: 3.8 KB, free: 412.5 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.12.108:29011 in memory (size: 3.8 KB, free: 413.4 MB)\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 80\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.12.108:29011 in memory (size: 71.5 KB, free: 413.5 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on jupyter-service-agaszmurlo:29011 in memory (size: 71.5 KB, free: 366.3 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.12.107:29011 in memory (size: 71.5 KB, free: 412.6 MB)\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 98\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 100\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 53\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 58\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 77\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 0\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 67\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 86\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 17\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 68\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 107\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 43\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 56\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_9_piece0 on jupyter-service-agaszmurlo:29011 in memory (size: 2.4 KB, free: 366.3 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.12.107:29011 in memory (size: 2.4 KB, free: 412.6 MB)\n",
      "21/01/15 14:36:23 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.12.108:29011 in memory (size: 2.4 KB, free: 413.5 MB)\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 29\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 40\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 69\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 48\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 76\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 65\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 106\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 84\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 96\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 63\n",
      "21/01/15 14:36:23 INFO ContextCleaner: Cleaned accumulator 27\n",
      "21/01/15 14:36:23 INFO HadoopFileSystemWrapper: Concatenating 3 parts to gs://edugen-lab-agaszmurlo/vcf/mother10.vcf\n",
      "21/01/15 14:36:23 INFO HadoopFileSystemWrapper: Concat not supported, merging serially\n",
      "21/01/15 14:36:29 INFO HadoopFileSystemWrapper: Concatenating to gs://edugen-lab-agaszmurlo/vcf/mother10.vcf done\n",
      "21/01/15 14:36:30 INFO SparkUI: Stopped Spark web UI at http://jupyter-service-agaszmurlo:4040\n",
      "21/01/15 14:36:30 INFO KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "21/01/15 14:36:30 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "21/01/15 14:36:30 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "21/01/15 14:36:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/01/15 14:36:30 INFO MemoryStore: MemoryStore cleared\n",
      "21/01/15 14:36:30 INFO BlockManager: BlockManager stopped\n",
      "21/01/15 14:36:30 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/01/15 14:36:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/01/15 14:36:30 INFO SparkContext: Successfully stopped SparkContext\n",
      "14:36:30.245 INFO  HaplotypeCallerSpark - Shutting down engine\n",
      "[January 15, 2021 2:36:30 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 2.04 minutes.\n",
      "Runtime.totalMemory()=509083648\n",
      "21/01/15 14:36:30 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/01/15 14:36:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-a7712177-2991-494b-bf43-791f17e6f7ba\n",
      "21/01/15 14:36:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-4378e17d-a49d-44f8-93d7-6797092e5fc7\n"
     ]
    }
   ],
   "source": [
    "! gatk-hpt-caller-k8s.sh \\\n",
    "  --conf \"spark.executor.memory=1g\" \\\n",
    "  --conf \"spark.driver.memory=1g\" \\\n",
    "  --conf \"spark.executor.instances=2\" \\\n",
    "  --conf \"spark.hadoop.fs.gs.block.size=8388608\" \\\n",
    "  -R /mnt/data/mapping/ref/ref.fasta \\\n",
    "  -I gs://edugen-lab-$USER/bam/mother10.bam \\\n",
    "  -O gs://edugen-lab-$USER/vcf/mother10.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://edugen-lab-agaszmurlo/vcf/\n",
      "gs://edugen-lab-agaszmurlo/vcf/mother.vcf\n",
      "gs://edugen-lab-agaszmurlo/vcf/mother10.vcf\n",
      "gs://edugen-lab-agaszmurlo/vcf/mother_anno.vcf\n",
      "gs://edugen-lab-agaszmurlo/vcf/mother_dec.vcf\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls gs://edugen-lab-$USER/vcf/  # sprawdzimy czy plik sie zapisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "echo \"####Running GATK with users' params: $@ on Kubernetes\"\n",
      "gatk HaplotypeCallerSpark \\\n",
      "  --spark-runner SPARK \\\n",
      "  --spark-master k8s://https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT \\\n",
      "  --conf spark.jars=/tmp/gcs-connector-${GCS_CONNECTOR_VERSION}-shaded.jar,/tmp/google-cloud-nio-${GCS_NIO_VERSION}-shaded.jar \\\n",
      "  --conf spark.hadoop.google.cloud.auth.service.account.enable=true \\\n",
      "  --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=$GOOGLE_APPLICATION_CREDENTIALS \\\n",
      "  --conf spark.kubernetes.driverEnv.GCS_PROJECT_ID=$PROJECT_ID \\\n",
      "  --conf spark.kubernetes.driverEnv.GOOGLE_APPLICATION_CREDENTIALS=$GOOGLE_APPLICATION_CREDENTIALS \\\n",
      "  --conf spark.executorEnv.GCS_PROJECT_ID=$PROJECT_ID \\\n",
      "  --conf spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS=$GOOGLE_APPLICATION_CREDENTIALS \\\n",
      "  --conf spark.kubernetes.driver.secrets.$SERVICE_ACCOUNT-secret=$SECRETS_MOUNT_DIR \\\n",
      "  --conf spark.kubernetes.executor.secrets.$SERVICE_ACCOUNT-secret=$SECRETS_MOUNT_DIR \\\n",
      "  --conf spark.hadoop.fs.gs.project.id=$PROJECT_ID \\\n",
      "  --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \\\n",
      "  --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \\\n",
      "  --conf spark.kubernetes.container.image=biodatageeks/spark-py:v2.4.3-edugen-0.1.7-gatk \\\n",
      "  --conf spark.kubernetes.authenticate.driver.serviceAccountName=$SERVICE_ACCOUNT \\\n",
      "  --conf spark.kubernetes.authenticate.serviceAccountName=$SERVICE_ACCOUNT \\\n",
      "  --conf spark.kubernetes.executor.podNamePrefix=gatk-exec-$USER \\\n",
      "  --conf spark.kubernetes.executor.label.spark-owner=$USER \\\n",
      "  --conf spark.kubernetes.executor.request.cores=0.4 \\\n",
      "  --conf spark.driver.port=29010 \\\n",
      "  --conf spark.blockManager.port=29011 \\\n",
      "  --conf spark.kubernetes.namespace=default \\\n",
      "  --conf spark.driver.host=jupyter-service-$USER \\\n",
      "  --conf spark.driver.bindAddress=$HOSTNAME \\\n",
      "  --conf spark.executorEnv.PYSPARK_PYTHON=$PYSPARK_PYTHON \\\n",
      "  \"$@\""
     ]
    }
   ],
   "source": [
    "! cat /usr/local/bin/gatk-hpt-caller-k8s.sh  # podejrzymy konfiguracje"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edugen",
   "language": "python",
   "name": "edugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
