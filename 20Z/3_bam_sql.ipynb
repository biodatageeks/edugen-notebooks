{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odczyty uliniowione\n",
    "\n",
    "W tej części zajmiemy się analiża uliniowionych odczytów (BAM) poprzez platformę Spark. Tym razem zamiast korzystać z API DataFrame posłużymy się językiem SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://edugen-lab-$USER/bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie sesji Spark\n",
    "Zainicjowanie sesji Spark oraz stworzenie schematu bazy danych z której będziemy korzystać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                               # moduł OS języka Python\n",
    "user_name = os.environ.get('USER')      # pobieramy zmienną środowiskową USER\n",
    "bucket = f\"gs://edugen-lab-{user_name}\" # konstruujemy sciezke dostepowa do pliku\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy korzystać z modułu Spark SQL. Możemy nasze dane traktować jako dane tabelaryczne. Zdefiniujmy tabelę.\n",
    "Tym razem korzystamy z DataSource -> BAMDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bam =  'alignments'\n",
    "\n",
    "alignments_path = f\"{bucket}/bam/mother10.bam\"\n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_bam}')\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_bam} \\\n",
    "USING org.biodatageeks.sequila.datasources.BAM.BAMDataSource \\\n",
    "OPTIONS(path \"{alignments_path}\")')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"describe {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby dostać się do danych w tabeli, bedziemy korzystać z język SQL (standard ANSI). \n",
    "Struktura polecen SQL jest nastepująca: \n",
    "\n",
    "```sql\n",
    "SELECT kolumny\n",
    "FROM tabela\n",
    "WHERE warunki\n",
    "GROUP BY wyrazenia grupujace\n",
    "HAVING warunki na grupy\n",
    "ORDER BY wyrazenie sortujące\n",
    "```\n",
    "Zapytanie może mieć podzapytania zagnieżdzone. W zapytaniu można korzystać ze złączeń z innymi tabeli poprzez JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wybór kolumn (klauzula SELECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select * from {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokaż unikalne wartości flag jakie przyjmują wartości z tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct flag from {table_bam} order by flag ASC\").show()  # distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Czym (jakim typem danych) są dane zwracane przez spark.sql (\"SELECT..\")?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sortowanie (klauzula ORDER BY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select distinct flag from {table_bam} order by flag ASC\") \n",
    "print(type(df))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrowanie wierszy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where pos < 3858310\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where pos < 3858310 and flag NOT IN (113,117,121) and mapq > 50\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Napisz zapytanie które zwróci unikalne wartości CIGAR na contigu 20 na pozycjach 1-1000000. Posortuj względem długości pola cigar malejąco. Jesli kilka CIGAR ma taką samą długość to posortuj rosnąco alfebatycznie. Ile jest takich unikalnych wartości?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct cigar from {table_bam} where pos between 1 and 1000000 and contig= '20' order by length(cigar) DESC, cigar ASC\").show()\n",
    "spark.sql(f\"select count (distinct cigar) from {table_bam} where pos between 1 and 1000000 and contig= '20'\").show()\n",
    "spark.sql(f\"select distinct cigar from {table_bam} where pos between 1 and 1000000 and contig= '20'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = spark.sql(f\"select distinct cigar from {table_bam} where pos between 1 and 1000000 and contig= '20'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = spark.sql(f\"select count (distinct cigar) from {table_bam} where pos between 1 and 1000000 and contig= '20'\").head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokaż ile jest wierszy o konkretnej wartości flagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select flag, count(*) as cnt from {table_bam} group by flag order by cnt desc\").show()  # AS - alias kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mozna wykorzystac kilka funkcji agregujących (na roznych kolumnach) w jednym zapytaniu. \n",
    "\n",
    "Pokaz ile odczytow ma dana flage. Pokaz takze ich srednia jakosc mapowania oraz minimalna pozycje na ktorej wystepuja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select flag, count(*) as cnt, avg(mapq) as avg_m, min(pos_start) as min from {table_bam} group by flag order by cnt, avg_m desc\").show()  # AS - alias kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcje mozemy zagniezdzac. Np: round(avg(kolumna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzycie funkcji floor na średniej\n",
    "spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min from {table_bam} group by flag order by cnt, avg_m desc\").show()  # AS - alias kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Wiedząc, że jako wyrażenie grupujące można wykorzystać wywołanie funkcji na kolumnie napisz zapytanie które policzy ile odczytów ma określoną długość CIGARa. Dla tych grup policz także zaokrągloną (ROUND) wartość jakości mapowania odczytów. Wynik zawęź tylko do odczytów występujących na chromosomie 20 lub 21. Posortuj malejaco liczności grup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select length(cigar), count(*) as cnt, round (avg(mapq)) as avg_map \\\n",
    "            from {table_bam} \\\n",
    "            where contig IN ('20', '21') \\\n",
    "            group by length(cigar) \\\n",
    "            order by cnt desc\").show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrowanie grup danych\n",
    "\n",
    "Warunki na pojedyncze wiersze nakładamy za pomocą klauzuli WHERE. Jeśli dokonujemy grupowania danych i chcemy w wynikach uzyskać jedynie informacje o grupach, które spełniają określone warunki używamy klauzuli HAVING. (HAVING moze wystapic wylacznie z GROUP BY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zrob podsumowanie dla grup odczytów o zgodnym flag. Policz srednie mapq oraz minmalna pozycje\n",
    "# Pokaz jedynie informacja o flagach dla ktorych jest ponad 1000 odczytów.\n",
    "spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min \\\n",
    "            from {table_bam} \\\n",
    "            group by flag \\\n",
    "            having count(*) > 1000 \\\n",
    "            order by cnt, avg_m desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Czy w klauzuli HAVING możemy użyc aliasu kolumny cnt?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warunki w HAVING też mogą być złożone (złączone AND, OR). Ale warunki w HAVING mogą się odnosić jedynie do kolumn grupujących lub funkcji agregujących"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min \\\n",
    "            from {table_bam} \\\n",
    "            group by flag \\\n",
    "            having count(*) > 1000 and flag != 99 \\\n",
    "            order by cnt, avg_m desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bledna konstrukcja - W HAVING mamy warunek na kolumny niegrupujace\n",
    "# spark.sql(f\"select flag, count(*) as cnt, floor(avg(mapq)) as avg_m, min(pos_start) as min \\\n",
    "#             from {table_bam} \\\n",
    "#             group by flag \\\n",
    "#             having count(*) > 1000 and sample_id ='mother' \\\n",
    "#             order by cnt, avg_m desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Wiedząc, że jako wyrażenie grupujące można wykorzystać wywołanie funkcji na kolumnie napisz zapytanie które policzy ile odczytów ma określoną długość CIGARa. \n",
    "Dla tych grup policz także zaokrągloną (ROUND) wartość jakości mapowania odczytów. \n",
    "Wynik zawęź tylko do odczytów występujących na chromosomie 20 lub 21. Pokaż tylko grupy mające średnia jakosc mapowania powyzej 30. Posortuj malejaco liczności grup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select length(cigar), count(*) as cnt, round (avg(mapq)) as avg_map \\\n",
    "            from {table_bam} \\\n",
    "            where contig IN ('20', '21') \\\n",
    "            group by length(cigar) \\\n",
    "            having avg(mapq) > 30 \\\n",
    "            order by cnt desc\").show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podzapytania nieskorelowane\n",
    "\n",
    "Chcemy zobaczyć, jaka jest najwyższa wartość mapq w naszym zbiorze danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select max(mapq) from {table_bam} \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy zatem jakie są dane tych odczytów które charakteryzują się najwyższą wartością jakosci mapowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where mapq=60 \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz sprobujmy to zapisac bez zaszywania wartości 60 w zapytaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where mapq=(select max(mapq) from {table_bam}) \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "\n",
    "1) Sprawdź ile jest odczytów mających maksymalna wartosc mapq. \n",
    "\n",
    "2) Zobacz ile jest odczytów ktorych mapq jest w pierwszej 3 wartosci mapq ze zbioru danych.\n",
    "\n",
    "* podpowiedzi: ograniczenie liczby zwracanych wierszy (LIMIT). \n",
    "* przemyślenie operatora porównania wartosci mapq oraz wartosci zwracanych przez podzapytanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where mapq=(select max(mapq) from {table_bam}) \").count())\n",
    "spark.sql(f\"select distinct mapq from {table_bam} order by mapq DESC LIMIT 5\").show()\n",
    "print(spark.sql(f\"select sample_id, contig, pos, mapq from {table_bam} where mapq IN (select distinct mapq from {table_bam} order by mapq DESC LIMIT 5) \").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie funkcji wbudowanych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niektóre funkcje do wykorzystania:\n",
    "ROUND, FLOOR, SIGN, POW, LEAST, LOG\n",
    "UPPER, LOWER, SUBSTR, \n",
    "NVL\n",
    "CURRENT_DATE\n",
    "MIN, MAX, SUM, STDDEV, AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"select distinct sample_id, upper(sample_id), current_date() from {table_bam} \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie funkcji rozszerzonych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Istnieją rozszerzenia Sparka do obslugi genomicznych danych. Na przykład pakiet sequila dostarcza metod do rozproszonego wyliczenia glebokosci pokrycia i pileup.\n",
    "\n",
    "http://biodatageeks.org/sequila/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeby skorzystac z rozszerzen importujemy modul i tworzymy obiekt ss, opakowanie na sesje sparkową\n",
    "from pysequila import SequilaSession\n",
    "ss = SequilaSession(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage jest dostępny jako funkcja tabelaryczna, czyli funkcja zwracająca tabelę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konstruujemy zapytanie\n",
    "cov_query = f\"select * from coverage ('{table_bam}', 'mother10', 'blocks')\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql(cov_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Pokaż tylko pozycje, które mają głębokość pokrycia większą niż 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql(f\"select * from coverage ('{table_bam}', 'mother10', 'blocks') where coverage >= 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "Korzystając z opcji uruchomienia funkcji coverage ktora zwraca pokrycie dla kazdej pozycji niezaleznie (bases zamiast blocks) napisz  zapytanie które zwróci ile pozycji ma daną głębokość pokrycia. Posortuj po liczności malejąco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql(f\"select coverage, count(*) as cnt from coverage ('{table_bam}', 'mother10', 'bases') group by coverage order by cnt desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykrywanie wariantów\n",
    "Na zajeciach skorzystamy z wersji rozproszonej HaplotypeCaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk-hpt-caller-k8s.sh \\\n",
    "  --conf \"spark.executor.memory=1g\" \\\n",
    "  --conf \"spark.driver.memory=1g\" \\\n",
    "  --conf \"spark.executor.instances=2\" \\\n",
    "  --conf \"spark.hadoop.fs.gs.block.size=8388608\" \\\n",
    "  -R /mnt/data/mapping/ref/ref.fasta \\\n",
    "  -I gs://edugen-lab-$USER/bam/mother10.bam \\\n",
    "  -O gs://edugen-lab-$USER/vcf/mother10.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://edugen-lab-$USER/vcf/  # sprawdzimy czy plik sie zapisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /usr/local/bin/gatk-hpt-caller-k8s.sh  # podejrzymy konfiguracje"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edugen",
   "language": "python",
   "name": "edugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
