{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie sesji Spark\n",
    "Zainicjowanie sesji Spark oraz stworzenie schematu bazy danych z której będziemy korzystać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobranie sesji Spark jest proste dla programisty korzystającego z notatnika, wymaga podania tylko kilku parametrów, ale faktyczna konfiguracja jest bardziej rozbudowana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "args = os.environ['PYSPARK_SUBMIT_ARGS'].replace(\"  \", \"\\n\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy pobraną sesję sparkową. Powstały dodatkowe pody gotowe na realizację obliczeń. A jak zwolnić te zasoby?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponownie pobieramy sesję Spark. Będziemy z niej korzystać. Po zakończonej pracy należy pamiętać o zastopowaniu sesji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odczyt danych\n",
    "Korzystając z sesji Spark można odczytać dane zapisane w lokalizacji dostępnej dla executorów (procesów obliczeniowych) koordynowanych przez Spark. \n",
    "Konieczne jest podanie ścieżki dostępowej do pliku i formatu danych (nie jest to jednoznazne z rozszerzeniem pliku).\n",
    "Dystrybucja Spark udostępnia kilka tzw Data Sources, które odczytują i zapisują dane w określonych formatach (CSV, formaty kolumnowe: parquet/orc).\n",
    "Data Sources zgodne z opracowanym interfejsem można samodzielnie tworzyć. Na dzisiejszych zajęciach będziemy korzystać z takich zdefiniowanych DS:\n",
    "* FASTQDataSource\n",
    "* BAMDataSource\n",
    "* VCFDataSource\n",
    "\n",
    "*UWAGA - powyższe sposoby odczytu plików nie są częścią głównej dystrybucji Spark. Wymagana jest dodatkowa konfiguracja.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                               # moduł OS języka Python\n",
    "user_name = os.environ.get('USER')      # pobieramy zmienną środowiskową USER\n",
    "bucket = f\"gs://edugen-lab-{user_name}\" # konstruujemy sciezke dostepowa do pliku\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`Co oznacza f przed cudzysłowem?`\n",
    "\n",
    "`Czy jest różnica między stosowaniem apostrofu i cudzysłowu przy definicji zmiennych przechowywujących łańcuchy znaków?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_path = f\"{bucket}/fastq/*\"  # * oznacza wszystkie pliki we wskazanej lokalizacji. Można podać konkretny plik\n",
    "fastq_all = spark.read.load(reads_path, format=\"org.biodatageeks.sequila.datasources.FASTQ.FASTQDataSource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fastq_all)      # jaki jest typ danych utworzonej zmiennej? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.printSchema() # jaki jest schemat danych?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fastq_all.columns)           # wymiary (liczba kolumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.explain(True)              # fizyczny plan wykonania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame jest abstrakcją nad innym typem danych (RDD), który jest podstawową rozproszoną strukturą danych. Poprzez DF możemy dostać się do rdd i zweryfikować na przykład liczbę partycji danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.rdd.getNumPartitions() # liczba partycji (bloków danych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podgląd danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.show(5)  # pierwsze 5 wierszy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.show(truncate=False) # bez skracania zawartości kolumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dostęp do wybranych danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacja projekcji (SELECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.select(\"sample_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Czy operacja select (sample_id) wpłynęła na fastq_all?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.printSchema()\n",
    "fastq_all.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli chcemy zachować wynik działania transformacji (w celu późniejszego wykorzystania) trzeba wynik zachować w zmiennej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_sample_only = fastq_all.select(\"sample_id\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_sample_only.printSchema()\n",
    "fastq_sample_only.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.select(\"sample_id\",\"seq\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powiedzmy, że interesują nas wszystkie kolumny poza qual. Jak to zrobic? Można wylistować wszystkie kolumny poza qual, ale to uciazliwe. Mozna skorzystac z operacji drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_no_qual=fastq_all.drop('qual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_no_qual.printSchema()\n",
    "fastq_no_qual.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "\n",
    "Stwórz nową ramkę danych która będzie zawierać wszystkie kolumny z fastq_all poza pos_x i pos_y. Wyświetl 10 wierszy z tej kolekcji pokazując pełną zawartość kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_cropped = fastq_all.drop('pos_x', 'pos_y')\n",
    "fastq_cropped.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wartości unikalne\n",
    "Jeśli chcemy uzyskać unikalne wartości w określonych kolumnach korzystamy z metody distinct().\n",
    "Operacje na DF można łańcuchowo łączyć, zatem na wyniku działania select() można wywołać kolejne metody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.select('sample_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.select('sample_id', 'filter_passed').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrowanie wynikow\n",
    "Nasz zbiór danych posiada odczyty z 3 próbek. Ograniczmy się do wybranych próbek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_mother = fastq_all.filter(\"sample_id = 'mother'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Czy już odbył się odczyt danych z fastq?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_mother.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_mother.select('sample_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_mother.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warunki można łączyć spójnikami logicznymi. Można używać operatorów arytmetycznych, przynależności do zbioru (IN), porównania znaków (LIKE), przyrównania do wartości NULL (IS NULL i IS NOT NULL)\n",
    "\n",
    "Przy korzystaniu z LIKE można użyć % jako oznaczenie dowolnego ciągu znaków"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokaż odczyty spełniające warunek ze nazwa instrumentu jest pusta, run_id jest >=0 a odczyt zaczyna sie od liter GCA. Pokaz tylko kolumny z filtra oraz nazwe probki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_mother.filter('instrument_name IS NULL and run_id>=0 and seq LIKE \"GCA%\"').select('sample_id', 'instrument_name', 'run_id', 'seq').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_mother.select('sample_id', 'instrument_name', 'run_id', 'seq').filter('instrument_name IS NULL and run_id>=0 and seq LIKE \"GCA%\"').show() # kolejnosc select i filter bez znaczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "\n",
    "Napisz polecenie które policzy ile jest rekordów dla próbki syna które spełniają warunki, że sekwencja odczytu konczy się na TGG a qual zaczyna się od ==."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.filter(\"sample_id='son' and seq LIKE '%TGG' and qual LIKE '==%' \").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uzywanie funkcji, kolumny wyliczane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.selectExpr(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.selectExpr(\"*\", \"length(seq) as len_seq\", \"length(qual) as len_qual\" ).show() ## dodanie dwóch kolumn wyliczanych przy uzyciu funkcji LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_fastq = fastq_all.selectExpr(\"*\", \"length(seq) as len_seq\", \"length(qual) as len_qual\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "extended_fastq.withColumn (\"date\", current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "fastq_all.withColumn(\"format\", lit('FASTQ')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> ZADANIE  </span>\n",
    "\n",
    "Napisz polecenie które zwróci ramkę danych zawierającą sklejenie wartości dwóch kolumn (sample_id) oraz daty eksperymentu (dodaj kolumne z wartościami 2019-01-15). W wynikach chcemy mieć tylko dane matki i ojca. Kolumny wynikowe: nazwa próbki, seq, qual, data eksperymentu oraz scalona nazwa probki oraz data eksperymenty (np father2019-01-15)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit, concat\n",
    "\n",
    "fastq_all.withColumn(\"experiment_date\", to_date(lit('2019-01-15')))\\\n",
    "        .withColumn(\"sample_date\", concat(\"sample_id\", \"experiment_date\"))\\\n",
    "        .select (\"sample_id\", \"sample_date\", \"seq\", \"qual\", \"experiment_date\")\\\n",
    "        .filter (\"sample_id IN ('mother', 'father')\")\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grupowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_all.groupBy(\"sample_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fastq_all.groupBy(\"sample_id\")) # to nie jest DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count=fastq_all.groupBy(\"sample_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count.orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapowanie do genomu referencyjnego\n",
    "Wykonamy mapowanie do genomu referencyjnych korzystając z rozproszenia danych miedzy procesy obliczeniowe sparka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowanie ścieżek do plików."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user_name = os.environ.get('USER')\n",
    "bucket = f\"gs://edugen-lab-{user_name}\"\n",
    "\n",
    "reads_file_path = f\"{bucket}/fastq/mother.fastq\"\n",
    "ref_path = \"/mnt/data/mapping/ref/ref.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konstruujemy komendę, która będzie uruchamiana na procesach obliczeniowych. Potrzebne narzędzia muszą być dostępne na węzłach obliczeniowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f'bwa mem -p {ref_path} - | samtools fixmate -m - - | samtools sort  | samtools markdup -r -S - -  | samtools addreplacerg  -r \"ID:S1\" -r \"SM:S1\"  -r \"PL:ILLUMINA\" - | samtools view -b -'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Żeby wykonać rozproszone obliczenia na danych genomicznych nalezy wykorzystac dodatkową bibliotekę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyseqtender import SeqTenderAlignment\n",
    "\n",
    "seq_aligner = SeqTenderAlignment(spark, reads_file_path, command)\n",
    "alignments_rdd = seq_aligner.pipe_reads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisujemy plik na kubełek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_file_path = f\"{bucket}/bam/mother10.bam\"\n",
    "seq_aligner.save_reads(bam_file_path, alignments_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://edugen-lab-$USER/bam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kończymy notatniki, należy zamknąć sesję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bam =  'alignments'\n",
    "\n",
    "alignments_path = f\"{bucket}/bam/mother.bam\"\n",
    "# alignments_path=\"gs://biodatageeks-ds-lab/genomics/bam/NA12878.proper.wes.chr1.md.bam\"\n",
    "ss.sql(f'DROP TABLE IF EXISTS {table_bam}')\n",
    "ss.sql(f'CREATE TABLE IF NOT EXISTS {table_bam} \\\n",
    "USING org.biodatageeks.sequila.datasources.BAM.BAMDataSource \\\n",
    "OPTIONS(path \"{alignments_path}\")')\n",
    "ss.sql(f\"select * from {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql(f\"select * from {table_bam}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"select * from coverage ('{table_bam}', 'mother', 'blocks')\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = \"/mnt/data/mappings/ref/ref.fasta\"\n",
    "query = f\"select * from pileup ('{table_bam}', 'mother', '{ref}', true)\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gatk-hpt-caller-k8s.sh \\\n",
    "  --conf \"spark.executor.memory=1g\" \\\n",
    "  --conf \"spark.driver.memory=1g\" \\\n",
    "  --conf \"spark.executor.instances=2\" \\\n",
    "  --conf \"spark.hadoop.fs.gs.block.size=8388608\" \\\n",
    "  -R /mnt/data/mapping/ref/ref.fasta \\\n",
    "  -I gs://edugen-lab-$USER/bam/mother6.bam \\\n",
    "  -O gs://edugen-lab-$USER/vcf/mother6.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://edugen-lab-$USER/vcf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_path=\"gs://edugen-lab-agaszmurlo/vcf/mother.vcf\"\n",
    "table_var = 'variants'\n",
    "ss.sql(f'DROP TABLE IF EXISTS {table_var}')\n",
    "ss.sql(f'CREATE TABLE IF NOT EXISTS {table_var} \\\n",
    "USING org.biodatageeks.sequila.datasources.VCF.VCFDataSource \\\n",
    "OPTIONS(path \"{var_path}\")')\n",
    "ss.sql(f\"select * from {table_var}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_path=\"gs://edugen-lab-agaszmurlo/vcf/mother.vcf\"\n",
    "\n",
    "from pyseqtender import SeqTenderAnnotation\n",
    "seq_anno = SeqTenderAnnotation(spark)\n",
    "dec_variants = seq_anno.pipe_variants (var_path, \"vt decompose -\")\n",
    "# dec_variants.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dec_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_variants.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dec_path = \"gs://edugen-lab-agaszmurlo/vcf/mother_dec.vcf\"\n",
    "seq_anno.save_variants (var_dec_path, dec_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dec_path=\"gs://edugen-lab-agaszmurlo/vcf/mother_dec.vcf\"\n",
    "table_var_dec = 'variants_norm'\n",
    "ss.sql(f'DROP TABLE IF EXISTS {table_var_dec}')\n",
    "ss.sql(f'CREATE TABLE IF NOT EXISTS {table_var_dec} \\\n",
    "USING org.biodatageeks.sequila.datasources.VCF.VCFDataSource \\\n",
    "OPTIONS(path \"{var_path}\")')\n",
    "ss.sql(f\"select * from {table_var_dec}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyseqtender import SeqTenderAnnotation\n",
    "seq_anno = SeqTenderAnnotation(spark)\n",
    "\n",
    "vcf_path=\"gs://edugen-lab-agaszmurlo/vcf/mother.vcf\"\n",
    "\n",
    "cache_dir = \"/mnt/data/annotation/102.0\"\n",
    "vep_version=\"102\"\n",
    "annotate_cmd = f\"\"\"vep --dir {cache_dir} --pick_allele --format vcf --no_stats --force_overwrite  --uniprot  -cache --vcf -offline \n",
    "        -o stdout \"\"\".replace(\"\\n   \", \"\")\n",
    "\n",
    "annotated = seq_anno.pipe_variants(vcf_path, annotate_cmd)\n",
    "#annotated.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_anno_path = \"gs://edugen-lab-agaszmurlo/vcf/mother_anno.vcf\"\n",
    "seq_anno.save_variants (var_anno_path, annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_anno_path=\"gs://edugen-lab-agaszmurlo/vcf/mother_anno.vcf\"\n",
    "table_var_anno= 'variants_anno'\n",
    "ss.sql(f'DROP TABLE IF EXISTS {table_var_anno}')\n",
    "ss.sql(f'CREATE TABLE IF NOT EXISTS {table_var_anno} \\\n",
    "USING org.biodatageeks.sequila.datasources.VCF.VCFDataSource \\\n",
    "OPTIONS(path \"{var_anno_path}\")')\n",
    "ss.sql(f\"select * from {table_var_anno} limit 10\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edugen",
   "language": "python",
   "name": "edugen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
